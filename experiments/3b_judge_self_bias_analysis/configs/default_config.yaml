# Judge Self-Bias Analysis Experiment Configuration

experiment:
  name: "judge_self_bias_analysis"
  description: "Analyze whether LLM judges show bias towards responses from the same model family"
  version: "1.0.0"
  author: "Multi-Judge Interpretability Team"

data:
  # Data loading and preprocessing settings
  sample_size: 10000  # Number of samples to use (reduced in quick mode)
  random_seed: 42     # Random seed for reproducibility
  min_judge_samples: 100  # Minimum samples per judge model
  min_response_samples: 100  # Minimum samples per response model
  
  # Column mappings (adjust based on your actual data structure)
  columns:
    judge_model: "judge_model"      # Column containing judge model identifiers
    response_model: "response_model" # Column containing response model identifiers
    judge_score: "judge_score"      # Column containing judge scores
    prompt_id: "prompt_id"          # Column containing prompt identifiers
    response_id: "response_id"      # Column containing response identifiers

analysis:
  # Bias detection settings
  confidence_level: 0.95        # Confidence level for statistical tests
  min_sample_size: 50           # Minimum sample size for analysis
  bias_threshold: 0.1           # Threshold for detecting bias (absolute score difference)
  
  # Model family classification
  model_families:
    GPT:
      - "gpt-3.5-turbo"
      - "gpt-4"
      - "gpt-4-turbo"
      - "gpt-4o"
    Claude:
      - "claude-3-sonnet"
      - "claude-3-opus"
      - "claude-3-haiku"
      - "claude-2.1"
    Llama:
      - "llama-2-7b"
      - "llama-2-13b"
      - "llama-2-70b"
      - "llama-3-8b"
      - "llama-3-70b"
    Gemini:
      - "gemini-pro"
      - "gemini-flash"
      - "gemini-1.5-pro"
    Other:
      - "other"  # Catch-all for unidentified models
  
  # Statistical analysis settings
  statistical_tests:
    - "t_test"
    - "confidence_intervals"
    - "effect_size"
  
  # Effect size thresholds
  effect_size_thresholds:
    negligible: 0.1
    small: 0.3
    medium: 0.5
    large: 0.7

output:
  # Output and visualization settings
  save_intermediate: true       # Save intermediate analysis results
  generate_plots: true          # Generate bias visualization plots
  save_raw_data: false         # Save raw analysis data
  
  # Plot settings
  plots:
    heatmap: true               # Generate bias heatmap
    bar_charts: true            # Generate bias comparison bar charts
    confidence_intervals: true  # Generate confidence interval plots
  
  # Report settings
  report_format: "markdown"     # Report output format
  include_statistics: true      # Include detailed statistics
  include_recommendations: true # Include actionable recommendations

logging:
  level: "INFO"                 # Logging level (DEBUG, INFO, WARNING, ERROR)
  save_logs: true               # Save logs to file
  log_file: "experiment.log"    # Log file name

# Quick mode overrides (applied when --quick flag is used)
quick_mode:
  sample_size: 1000            # Reduced sample size for quick testing
  min_sample_size: 25          # Lower minimum sample requirements
  generate_plots: false         # Skip plot generation for speed
  save_intermediate: false      # Skip intermediate saves
