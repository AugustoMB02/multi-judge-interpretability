{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from martian_apart_hack_sdk import exceptions, judge_specs, martian_client, utils\n",
    "from martian_apart_hack_sdk.models import judge_evaluation, llm_models, router_constraints\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from openai.types.chat import (\n",
    "    chat_completion,\n",
    "    chat_completion_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.load_config()\n",
    "client = martian_client.MartianClient(\n",
    "    api_url=config.api_url,\n",
    "    api_key=config.api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_IDS = [\n",
    "    'harmlessness-judge',\n",
    "    'privacy-judge',\n",
    "    'factual-accuracy-judge',\n",
    "    'prompt-faithfulness-relevance-judge',\n",
    "    'calibration-uncertainty-judge',\n",
    "    'bias-fairness-judge',\n",
    "    'reasoning-consistency-judge',\n",
    "    'discourse-coherence-judge',\n",
    "    'conciseness-redundancy-judge',\n",
    "    'style-formatting-judge',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGES = { id: client.judges.get(judge_id=id) for id in JUDGE_IDS }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(query, answer, judge):\n",
    "    # Run the judge spec.\n",
    "    chat_request_text = query\n",
    "    chat_response_text = answer\n",
    "\n",
    "    completion_request = {\n",
    "        \"model\": llm_models.GPT_4O_MINI,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": chat_request_text}],\n",
    "    }\n",
    "\n",
    "    chat_completion_response = chat_completion.ChatCompletion(\n",
    "        id=\"123\",\n",
    "        choices=[\n",
    "            chat_completion.Choice(\n",
    "                finish_reason=\"stop\",\n",
    "                index=0,\n",
    "                message=chat_completion_message.ChatCompletionMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=chat_response_text,\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        created=0,\n",
    "        model=\"gpt-4o\",\n",
    "        object=\"chat.completion\",\n",
    "        service_tier=None,\n",
    "    )\n",
    "\n",
    "    evaluation_result = client.judges.evaluate(\n",
    "        judge,\n",
    "        completion_request=completion_request,\n",
    "        completion_response=chat_completion_response,\n",
    "    )\n",
    "\n",
    "    return evaluation_result.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = '9000.pkl'\n",
    "\n",
    "with open(df_path, 'rb') as f:\n",
    "    df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_judge(args: Tuple[str, str, str, dict]) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a single judge - designed for parallel execution\n",
    "    \n",
    "    Args:\n",
    "        args: Tuple of (question, answer, judge_id, judges_dict)\n",
    "    Returns:\n",
    "        Tuple of (judge_index, score)\n",
    "    \"\"\"\n",
    "    question, answer, judge_id, judges = args\n",
    "    judge_index = JUDGE_IDS.index(judge_id)\n",
    "    score = get_score(question, answer, judges[judge_id])\n",
    "    return (judge_index, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_backoff(args, max_retries: int = 5, initial_delay: float = 1.0):\n",
    "    \"\"\"\n",
    "    Wrap evaluate_single_judge with exponential back-off on exception.\n",
    "    \n",
    "    Args:\n",
    "        args: Tuple of (question, answer, judge_id, JUDGES)\n",
    "        max_retries: Number of retry attempts\n",
    "        initial_delay: Seconds to sleep before first retry\n",
    "    \"\"\"\n",
    "    delay = initial_delay\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return evaluate_single_judge(args)\n",
    "        except Exception:\n",
    "            if attempt == max_retries - 1:\n",
    "                # Last attempt: re-raise or return a default\n",
    "                raise\n",
    "            time.sleep(delay)\n",
    "            delay *= 2\n",
    "    # Fallback (should not reach here)\n",
    "    return evaluate_single_judge(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair_parallel(question: str, answer: str, max_workers: int = None) -> List[float]:\n",
    "    \"\"\"\n",
    "    Parallelize judge evaluations using ThreadPoolExecutor,\n",
    "    with exponential back-off on each judge call.\n",
    "    \n",
    "    Args:\n",
    "        question: The query text\n",
    "        answer: The response text\n",
    "        max_workers: Number of parallel workers (None = auto-select based on CPU)\n",
    "    \n",
    "    Returns:\n",
    "        List of scores in judge order\n",
    "    \"\"\"\n",
    "    scores = [0.0] * len(JUDGE_IDS)\n",
    "    eval_args = [(question, answer, judge_id, JUDGES) for judge_id in JUDGE_IDS]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for judge_index, score in executor.map(evaluate_with_backoff, eval_args):\n",
    "            scores[judge_index] = score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_row(args):\n",
    "    idx, question, answer = args\n",
    "    return idx, evaluate_pair_parallel(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100 rows! Saving progress...\n",
      "Completed 200 rows! Saving progress...\n",
      "Completed 300 rows! Saving progress...\n",
      "Completed 400 rows! Saving progress...\n",
      "Completed 500 rows! Saving progress...\n",
      "Completed 600 rows! Saving progress...\n",
      "Completed 700 rows! Saving progress...\n",
      "Completed 800 rows! Saving progress...\n",
      "Completed 900 rows! Saving progress...\n",
      "Completed 1000 rows! Saving progress...\n",
      "Completed 1100 rows! Saving progress...\n",
      "Completed 1200 rows! Saving progress...\n",
      "Completed 1300 rows! Saving progress...\n",
      "Completed 1400 rows! Saving progress...\n",
      "Completed 1500 rows! Saving progress...\n",
      "Completed 1600 rows! Saving progress...\n",
      "Completed 1700 rows! Saving progress...\n",
      "Completed 1800 rows! Saving progress...\n",
      "Completed 1900 rows! Saving progress...\n",
      "Completed 2000 rows! Saving progress...\n",
      "Completed 2100 rows! Saving progress...\n",
      "Completed 2200 rows! Saving progress...\n",
      "Completed 2300 rows! Saving progress...\n",
      "Completed 2400 rows! Saving progress...\n",
      "Completed 2500 rows! Saving progress...\n",
      "Completed 2600 rows! Saving progress...\n",
      "Completed 2700 rows! Saving progress...\n",
      "Completed 2800 rows! Saving progress...\n",
      "Completed 2900 rows! Saving progress...\n",
      "Completed 3000 rows! Saving progress...\n",
      "Completed 3100 rows! Saving progress...\n",
      "Completed 3200 rows! Saving progress...\n",
      "Completed 3300 rows! Saving progress...\n",
      "Completed 3400 rows! Saving progress...\n",
      "Completed 3500 rows! Saving progress...\n",
      "Completed 3600 rows! Saving progress...\n",
      "Completed 3700 rows! Saving progress...\n",
      "Completed 3800 rows! Saving progress...\n",
      "Completed 3900 rows! Saving progress...\n",
      "Completed 4000 rows! Saving progress...\n",
      "Completed 4100 rows! Saving progress...\n"
     ]
    }
   ],
   "source": [
    "# Prepare argument list\n",
    "completed = 0\n",
    "batch = 0\n",
    "\n",
    "results = [None] * len(df)\n",
    "tasks = [\n",
    "    (i, row[\"instruction\"], row[\"answer\"])\n",
    "    for i, (_, row) in enumerate(df.iterrows())\n",
    "]\n",
    "\n",
    "# Parallelize across rows\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for i, scores in executor.map(eval_row, tasks):\n",
    "        results[i] = scores\n",
    "        completed += 1\n",
    "\n",
    "        # Kinky, really dirty stuff\n",
    "        if completed % 100 == 0:\n",
    "            print(f\"Completed {completed} rows! Saving progress...\")\n",
    "            \n",
    "            with open(f'progress_{batch}.pkl', 'wb') as f:\n",
    "                pickle.dump(results, f)\n",
    "\n",
    "            batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('progress_40.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scores'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uuid\n",
       "8d76e6c7-c0d4-4a85-8b04-a0487951b9c4           [4, 3.8, 1.5, 2, 1.5, 3.5, 3, 3.2, 2.5, 3]\n",
       "45845924-5386-428d-bf13-10b66ea048b4       [4, 4, 2.7, 2.8, 1.5, 3.5, 2.5, 3.5, 2.5, 2.5]\n",
       "2b1db340-dd68-495d-bcf5-43f5479dfe10       [3.5, 4, 3.5, 2, 2.5, 3.8, 3.5, 3.5, 2.5, 2.5]\n",
       "ea344eb0-64bc-4266-b970-0ebdacd2d661             [2, 3, 1, 0.9, 0.5, 2, 0.8, 1, 0.9, 0.9]\n",
       "32758846-9750-4eac-8702-8c4dcf215409    [3.5, 3.8, 3.4, 3.8, 3.5, 3.5, 3.5, 3.5, 2.8, ...\n",
       "                                                              ...                        \n",
       "24eb39a7-ca81-4ce5-a54c-fe52e79ad0af                                                 None\n",
       "92897240-2314-4ba4-8b5e-547599284272                                                 None\n",
       "546a94e9-f1f0-4864-86d7-bb4605a3d28e                                                 None\n",
       "3efcb49d-5840-467f-b991-a30894d59628                                                 None\n",
       "e48486fe-b8ca-4066-b4ce-c856f747a17c                                                 None\n",
       "Name: scores, Length: 9000, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df[df['scores'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('4100.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
