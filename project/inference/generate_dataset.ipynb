{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We annotate the data with judges' scores once we get the 'hu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from martian_apart_hack_sdk import martian_client, utils\n",
    "from martian_apart_hack_sdk.models import llm_models\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "\n",
    "from openai.types.chat import (\n",
    "    chat_completion,\n",
    "    chat_completion_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.load_config()\n",
    "client = martian_client.MartianClient(\n",
    "    api_url=config.api_url,\n",
    "    api_key=config.api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_IDS = [\n",
    "    'harmlessness-judge',\n",
    "    'privacy-judge',\n",
    "    'factual-accuracy-judge',\n",
    "    'prompt-faithfulness-relevance-judge',\n",
    "    'calibration-uncertainty-judge',\n",
    "    'bias-fairness-judge',\n",
    "    'reasoning-consistency-judge',\n",
    "    'discourse-coherence-judge',\n",
    "    'conciseness-redundancy-judge',\n",
    "    'style-formatting-judge',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGES = { id: client.judges.get(judge_id=id) for id in JUDGE_IDS }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(query, answer, judge):\n",
    "    # Get the score from the judge\n",
    "    chat_request_text = query\n",
    "    chat_response_text = answer\n",
    "\n",
    "    completion_request = {\n",
    "        \"model\": llm_models.GPT_4O_MINI,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": chat_request_text}],\n",
    "    }\n",
    "\n",
    "    chat_completion_response = chat_completion.ChatCompletion(\n",
    "        id=\"123\",\n",
    "        choices=[\n",
    "            chat_completion.Choice(\n",
    "                finish_reason=\"stop\",\n",
    "                index=0,\n",
    "                message=chat_completion_message.ChatCompletionMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=chat_response_text,\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        created=0,\n",
    "        model=\"gpt-4o\",\n",
    "        object=\"chat.completion\",\n",
    "        service_tier=None,\n",
    "    )\n",
    "\n",
    "    evaluation_result = client.judges.evaluate(\n",
    "        judge,\n",
    "        completion_request=completion_request,\n",
    "        completion_response=chat_completion_response,\n",
    "    )\n",
    "\n",
    "    return evaluation_result.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_judge(args: Tuple[str, str, str, dict]) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a single judge - designed for parallel execution\n",
    "    \n",
    "    Args:\n",
    "        args: Tuple of (question, answer, judge_id, judges_dict)\n",
    "    Returns:\n",
    "        Tuple of (judge_index, score)\n",
    "    \"\"\"\n",
    "    question, answer, judge_id, judges = args\n",
    "    judge_index = JUDGE_IDS.index(judge_id)\n",
    "    score = get_score(question, answer, judges[judge_id])\n",
    "    return (judge_index, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_backoff(args, max_retries: int = 5, initial_delay: float = 1.0):\n",
    "    \"\"\"\n",
    "    Wrap evaluate_single_judge with exponential back-off on exception.\n",
    "    \n",
    "    Args:\n",
    "        args: Tuple of (question, answer, judge_id, JUDGES)\n",
    "        max_retries: Number of retry attempts\n",
    "        initial_delay: Seconds to sleep before first retry\n",
    "    \"\"\"\n",
    "    delay = initial_delay\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return evaluate_single_judge(args)\n",
    "        except Exception:\n",
    "            if attempt == max_retries - 1:\n",
    "                # Last attempt: re-raise or return a default\n",
    "                raise\n",
    "            time.sleep(delay)\n",
    "            delay *= 2\n",
    "    # Fallback (should not reach here)\n",
    "    return evaluate_single_judge(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair_parallel(question: str, answer: str, max_workers: int = None) -> List[float]:\n",
    "    \"\"\"\n",
    "    Parallelize judge evaluations using ThreadPoolExecutor,\n",
    "    with exponential back-off on each judge call.\n",
    "    \n",
    "    Args:\n",
    "        question: The query text\n",
    "        answer: The response text\n",
    "        max_workers: Number of parallel workers (None = auto-select based on CPU)\n",
    "    \n",
    "    Returns:\n",
    "        List of scores in judge order\n",
    "    \"\"\"\n",
    "    scores = [0.0] * len(JUDGE_IDS)\n",
    "    eval_args = [(question, answer, judge_id, JUDGES) for judge_id in JUDGE_IDS]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for judge_index, score in executor.map(evaluate_with_backoff, eval_args):\n",
    "            scores[judge_index] = score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_row(args):\n",
    "    idx, question, answer = args\n",
    "    return idx, evaluate_pair_parallel(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_rows(df):\n",
    "    completed = 0\n",
    "    batch = 0\n",
    "\n",
    "    # This is the trick to parallelize the code across rows and allow for \"resuming\" from a checkpoint\n",
    "    results = [None] * len(df)\n",
    "    tasks = [\n",
    "        (i, row[\"instruction\"], row[\"answer\"])\n",
    "        for i, (_, row) in enumerate(df.iterrows())\n",
    "    ]\n",
    "\n",
    "    # Parallelize across rows\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for i, scores in executor.map(eval_row, tasks):\n",
    "            results[i] = scores\n",
    "            completed += 1\n",
    "\n",
    "            # This is a really kinky way to save progress but it worked :p\n",
    "            if completed % 100 == 0:\n",
    "                print(f\"Completed {completed} rows! Saving progress...\")\n",
    "                \n",
    "                # The so-called \"progress\" files\n",
    "                with open(f'progress_{batch}.pkl', 'wb') as f:\n",
    "                    pickle.dump(results, f)\n",
    "\n",
    "                batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('questions_and_answers.pkl', 'rb') as f:\n",
    "    questions_and_answers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, the code below should eventually stop. If for some reason it stops, you can use the progress files to filter the rows that are yet to be computed and then call `label_rows` again over that subset dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_rows(questions_and_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how big your dataset is, you will have to load a different progress file. Since we loaded 10K human simulated annotated records, then we load progress file **#99**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('progress_99.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Cardinalities between the questions_and_answers and the results should match\n",
    "questions_and_answers['scores'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then save!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('qas_with_scores.pkl', 'wb') as f:\n",
    "    pickle.dump(questions_and_answers, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
